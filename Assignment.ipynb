{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computational Linguistics Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "https://github.com/ollie283/language-models/blob/master/LangModel.py\n",
    "https://medium.com/@davidmasse8/predicting-the-next-word-back-off-language-modeling-8db607444ba9\n",
    "https://www.aclweb.org/anthology/D07-1090.pdf\n",
    "https://github.com/elainexmas/NLP/blob/master/StupidBackoffLanguageModel.py\n",
    "\n",
    "\n",
    "QUESTIONS:\n",
    "- what does the output look like?\n",
    "- LM class, what's the smooth argument?\n",
    "- last box of code has class Corpus with extra argument (pad = True), but we can't change the corpus??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda3/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:17: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3,and in 3.9 it will stop working\n",
      "  from collections import Mapping, defaultdict\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import nltk.lm as lm\n",
    "from nltk.util import bigrams, everygrams\n",
    "from collections import Counter, defaultdict\n",
    "from nltk.lm.preprocessing import pad_both_ends, flatten, padded_everygram_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nwith open(train_path,'r') as infile:\\n    o = json.load(infile)\\n    chunkSize = 50000\\n    for i in range(0, len(o), chunkSize):\\n        with open(outpath + '_' + str(i//chunkSize) + '.json', 'w') as outfile:\\n            json.dump(o[i:i+chunkSize], outfile)\\n            \\n\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_path = 'training_corpus.json'\n",
    "\n",
    "outpath = '/Users/ronjaronnback/Desktop/University/Year_3/Semester 2/Computational Linguistics/Assignment/'\n",
    "\n",
    "import json\n",
    "import sys\n",
    "\n",
    "# ONLY FOR SPLITTING TRAINING FILE\n",
    "\n",
    "\"\"\"\n",
    "with open(train_path,'r') as infile:\n",
    "    o = json.load(infile)\n",
    "    chunkSize = 50000\n",
    "    for i in range(0, len(o), chunkSize):\n",
    "        with open(outpath + '_' + str(i//chunkSize) + '.json', 'w') as outfile:\n",
    "            json.dump(o[i:i+chunkSize], outfile)\n",
    "            \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = '_0.json'\n",
    "test_path = '_1.json'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Corpus(object):\n",
    "    \n",
    "    \"\"\"\n",
    "    This class creates a corpus object read off a .json file consisting of a list of lists,\n",
    "    where each inner list is a sentence encoded as a list of strings.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, path, t, n, bos_eos=True, vocab=None):\n",
    "        \n",
    "        \"\"\"\n",
    "        DON'T TOUCH THIS CLASS! IT'S HERE TO SHOW THE PROCESS, YOU DON'T NEED TO ANYTHING HERE. \n",
    "        \n",
    "        A Corpus object has the following attributes:\n",
    "         - vocab: set or None (default). If a set is passed, words in the input .json file not \n",
    "                         found in the set are replaced with the UNK string\n",
    "         - path: str, the path to the .json file used to build the corpus object\n",
    "         - t: int, words with a lower frequency count than t are replaced with the UNK string\n",
    "         - ngram_size: int, 2 for bigrams, 3 for trigrams, and so on.\n",
    "         - bos_eos: bool, default to True. If False, bos and eos symbols are not prepended and appended to sentences.\n",
    "         - sentences: list of lists, containing the input sentences after lowercasing and \n",
    "                         splitting at the white space\n",
    "         - frequencies: Counter, mapping tokens to their frequency count in the corpus\n",
    "        \"\"\"\n",
    "        \n",
    "        self.vocab = vocab        \n",
    "        self.path = path\n",
    "        self.t = t\n",
    "        self.ngram_size = n\n",
    "        self.bos_eos = bos_eos\n",
    "        \n",
    "        # input --> [['I am home.'], ['You went to the park.'], ...]\n",
    "        self.sentences = self.read()\n",
    "        # output --> [['i', 'am', 'home' '.'], ['you', 'went', 'to', 'the', 'park', '.'], ...]\n",
    "    \n",
    "        self.frequencies = self.freq_distr()\n",
    "        # output --> Counter('the': 485099, 'of': 301877, 'i': 286549, ...)\n",
    "        # the numbers are made up, they aren't the actual frequency counts\n",
    "        \n",
    "        if self.t or self.vocab:\n",
    "            # output --> [['i', 'am', 'home' '.'], ['you', 'went', 'to', 'the', 'park', '.'], ...]\n",
    "            self.sentences = self.filter_words()\n",
    "            # output --> [['i', 'am', 'home' '.'], ['you', 'went', 'to', 'the', 'UNK', '.'], ...]\n",
    "            # supposing that park wasn't frequent enough or was outside of the training vocabulary, it gets\n",
    "            # replaced by the UNK string\n",
    "            \n",
    "        if self.bos_eos:\n",
    "            # output --> [['i', 'am', 'home' '.'], ['you', 'went', 'to', 'the', 'park', '.'], ...]\n",
    "            self.sentences = self.add_bos_eos()\n",
    "            # output --> [['eos', i', 'am', 'home' '.', 'bos'], ['eos', you', 'went', 'to', 'the', 'park', '.', 'bos'], ...]\n",
    "                    \n",
    "    def read(self):\n",
    "        \n",
    "        \"\"\"\n",
    "        Reads the sentences off the .json file, replaces quotes, lowercases strings and splits \n",
    "        at the white space. Returns a list of lists.\n",
    "        \"\"\"\n",
    "        \n",
    "        sentences = []\n",
    "        with open(self.path, 'r', encoding='latin-1') as f:\n",
    "            for line in f:\n",
    "                # first strip away newline symbols and the like, then replace ' and \" with the empty string and\n",
    "                # get rid of possible remaining trailing spaces \n",
    "                line = line.strip().translate({ord(i): None for i in '\"\\'\\\\'}).strip(' ')\n",
    "                # lowercase and split at the white space (the corpus has ben previously tokenized)\n",
    "                sentences.append(line.lower().split(' '))\n",
    "        \n",
    "        return sentences\n",
    "    \n",
    "    def freq_distr(self):\n",
    "        \n",
    "        \"\"\"\n",
    "        Creates a counter mapping tokens to frequency counts\n",
    "        \n",
    "        count = Counter()\n",
    "        for sentence in self.sentences:\n",
    "            for word in sentence:\n",
    "                count[w] += 1\n",
    "            \n",
    "        \"\"\"\n",
    "    \n",
    "        return Counter([word for sentence in self.sentences for word in sentence])\n",
    "        \n",
    "    \n",
    "    def filter_words(self):\n",
    "        \n",
    "        \"\"\"\n",
    "        Replaces illegal tokens with the UNK string. A token is illegal if its frequency count\n",
    "        is lower than the given threshold and/or if it falls outside the specified vocabulary.\n",
    "        The two filters can be both active at the same time but don't have to be. To exclude the \n",
    "        frequency filter, set t=0 in the class call.\n",
    "        \"\"\"\n",
    "        \n",
    "        filtered_sentences = []\n",
    "        for sentence in self.sentences:\n",
    "            filtered_sentence = []\n",
    "            for word in sentence:\n",
    "                if self.t and self.vocab:\n",
    "                    # check that the word is frequent enough and occurs in the vocabulary\n",
    "                    filtered_sentence.append(\n",
    "                        word if self.frequencies[word] > self.t and word in self.vocab else 'UNK'\n",
    "                    )\n",
    "                else:\n",
    "                    if self.t:\n",
    "                        # check that the word is frequent enough\n",
    "                        filtered_sentence.append(word if self.frequencies[word] > self.t else 'UNK')\n",
    "                    else:\n",
    "                        # check if the word occurs in the vocabulary\n",
    "                        filtered_sentence.append(word if word in self.vocab else 'UNK')\n",
    "                        \n",
    "            if len(filtered_sentence) > 1:\n",
    "                # make sure that the sentence contains more than 1 token\n",
    "                filtered_sentences.append(filtered_sentence)\n",
    "    \n",
    "        return filtered_sentences\n",
    "    \n",
    "    def add_bos_eos(self): # PADDING ADDED HERE\n",
    "        \n",
    "        \"\"\"\n",
    "        Adds the necessary number of BOS symbols and one EOS symbol.\n",
    "        \n",
    "        In a bigram model, you need on bos and one eos; in a trigram model you need two bos and one eos, and so on...\n",
    "        \"\"\"\n",
    "        \n",
    "        padded_sentences = []\n",
    "        for sentence in self.sentences:\n",
    "            padded_sentence = ['#bos#']*(self.ngram_size-1) + sentence + ['#eos#']\n",
    "            padded_sentences.append(padded_sentence)\n",
    "    \n",
    "        return padded_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LM(object):\n",
    "    \n",
    "    \"\"\"\n",
    "    Creates a language model object which can be trained and tested.\n",
    "    The language model has the following attributes:\n",
    "     - vocab: set of strings\n",
    "     - lam: float, indicating the constant to add to transition counts to smooth them (default to 1)\n",
    "     - ngram_size: int, the size of the ngrams\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n, vocab=None, smooth='Laplace', lam=1):\n",
    "        \n",
    "        self.vocab = vocab\n",
    "        self.lam = lam\n",
    "        self.ngram_size = n\n",
    "        self.frequencies = []\n",
    "        \n",
    "    def get_ngram(self, sentence, i):\n",
    "        \n",
    "        \"\"\"\n",
    "        CHANGE AT OWN RISK.\n",
    "        \n",
    "        Takes in a list of string and an index, and returns the history and current \n",
    "        token of the appropriate size: the current token is the one at the provided \n",
    "        index, while the history consists of the n-1 previous tokens. If the ngram \n",
    "        size is 1, only the current token is returned.\n",
    "        \n",
    "        Example:\n",
    "        input sentence: ['bos', 'i', 'am', 'home', '']\n",
    "        target index: 2\n",
    "        ngram size: 3\n",
    "        \n",
    "        ngram = ['bos', 'i', 'am']  \n",
    "        #from index 2-(3-1) = 0 to index i (the +1 is just because of how Python slices lists) \n",
    "        \n",
    "        history = ('bos', 'i')\n",
    "        target = 'am'\n",
    "        return (('bos', 'i'), 'am')\n",
    "        \"\"\"\n",
    "        \n",
    "        if self.ngram_size == 1:\n",
    "            return sentence[i]\n",
    "        else:\n",
    "            ngram = sentence[i-(n-1):i+1]\n",
    "            history = tuple(ngram[:-1])\n",
    "            target = ngram[-1]\n",
    "            return (history, target)\n",
    "                    \n",
    "    def update_counts(self, corpus):\n",
    "        \n",
    "        \"\"\"\n",
    "        CHANGE AT OWN RISK.\n",
    "        \n",
    "        Creates a transition matrix with counts in the form of a default dict mapping history\n",
    "        states to current states to the co-occurrence count (unless the ngram size is 1, in which\n",
    "        case the transition matrix is a simple counter mapping tokens to frequencies. \n",
    "        The ngram size of the corpus object has to be the same as the language model ngram size.\n",
    "        The input corpus (passed by providing the corpus object) is processed by extracting ngrams\n",
    "        of the chosen size and updating transition counts.\n",
    "        \n",
    "        This method creates three attributes for the language model object:\n",
    "         - counts: dict, described above\n",
    "         - vocab: set, containing all the tokens in the corpus\n",
    "         - vocab_size: int, indicating the number of tokens in the vocabulary\n",
    "        \"\"\"\n",
    "        # ADDED\n",
    "        self.frequencies = corpus.frequencies\n",
    "        \n",
    "        if self.ngram_size != corpus.ngram_size:\n",
    "            raise ValueError(\"The corpus was pre-processed considering an ngram size of {} while the \"\n",
    "                             \"language model was created with an ngram size of {}. \\n\"\n",
    "                             \"Please choose the same ngram size for pre-processing the corpus and fitting \"\n",
    "                             \"the model.\".format(corpus.ngram_size, self.ngram_size))\n",
    "        \n",
    "        self.counts = defaultdict(dict) if self.ngram_size > 1 else Counter()\n",
    "        for sentence in corpus.sentences:\n",
    "            for idx in range(self.ngram_size-1, len(sentence)):\n",
    "                ngram = self.get_ngram(sentence, idx)\n",
    "                if self.ngram_size == 1:\n",
    "                    self.counts[ngram] += 1\n",
    "                else:\n",
    "                    # it's faster to try to do something and catch an exception than to use an if statement to check\n",
    "                    # whether a condition is met beforehand. The if is checked everytime, the exception is only catched\n",
    "                    # the first time, after that everything runs smoothly\n",
    "                    try:\n",
    "                        self.counts[ngram[0]][ngram[1]] += 1\n",
    "                    except KeyError:\n",
    "                        self.counts[ngram[0]][ngram[1]] = 1\n",
    "        \n",
    "        # first loop through the sentences in the corpus, than loop through each word in a sentence\n",
    "        self.vocab = {word for sentence in corpus.sentences for word in sentence}\n",
    "        self.vocab_size = len(self.vocab)\n",
    "    \n",
    "    def get_unigram_probability(self, ngram):\n",
    "        \n",
    "        \"\"\"\n",
    "        CHANGE THIS.\n",
    "        \n",
    "        Compute the probability of a given unigram in the estimated language model using\n",
    "        Laplace smoothing (add k).\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        tot = sum(list(self.counts.values())) + (self.vocab_size*self.lam)\n",
    "        try:\n",
    "            ngram_count = self.counts[ngram] + self.lam\n",
    "        except KeyError:\n",
    "            ngram_count = self.lam\n",
    "            print(ngram_count, tot)\n",
    "        \n",
    "        return ngram_count/tot\n",
    "    \n",
    "    def get_ngram_probability(self, history, target):\n",
    "        \n",
    "        \"\"\"\n",
    "        CHANGE THIS.\n",
    "        \n",
    "        Compute the conditional probability of the target token given the history, using \n",
    "        Laplace smoothing (add k).\n",
    "        \n",
    "        STUPID BACKOFF\n",
    "        \n",
    "        alpha = backoff factor\n",
    "        \n",
    "        if frequency(targetword) > 0:\n",
    "             return frequency(targetword) / frequency(history)\n",
    "            \n",
    "        else:\n",
    "            return alpha * S(targetword - 1 | history of that)\n",
    "        \n",
    "        \"\"\"\n",
    "        num_backoffs = 0\n",
    "        n_gram_length = len(history)\n",
    "        whats_left = n_gram_length - num_backoffs\n",
    "        \n",
    "        try:\n",
    "            if whats_left < 1:\n",
    "                print(\"Backing off\")\n",
    "                return 0.4**num_backoffs * get_unigram_probability(target)        \n",
    "\n",
    "            freq_target = np.sum(list(self.counts[history].values()))\n",
    "            #print(\"Freq of target word: \", freq_target)\n",
    "\n",
    "            freq_history = self.counts[history][target]\n",
    "            #print(\"Freq of prefix: \", freq_history)\n",
    "\n",
    "            return freq_target / freq_history\n",
    "        \n",
    "        except KeyError:\n",
    "            return self.lam/self.vocab_size*self.lam\n",
    "        \n",
    "        #OLD CODE\n",
    "        \"\"\"\n",
    "        try:\n",
    "            ngram_tot = np.sum(list(self.counts[history].values())) + (self.vocab_size*self.lam)\n",
    "            try:\n",
    "                transition_count = self.counts[history][target] + self.lam\n",
    "            except KeyError:\n",
    "                transition_count = self.lam\n",
    "        except KeyError: #\n",
    "            transition_count = self.lam\n",
    "            ngram_tot = self.vocab_size*self.lam\n",
    "            \n",
    "        return transition_count/ngram_tot\"\"\"\n",
    "        \n",
    "    \n",
    "    def perplexity(self, corpus):\n",
    "        \n",
    "        \"\"\"\n",
    "        Uses the estimated language model to process a corpus and computes the perplexity \n",
    "        of the language model over the corpus.\n",
    "        \n",
    "        DON'T TOUCH THIS FUNCTION!!!\n",
    "        \"\"\"\n",
    "        \n",
    "        entropy = []\n",
    "        for sentence in corpus.sentences:\n",
    "            for idx in range(self.ngram_size-1, len(sentence)):\n",
    "                ngram = self.get_ngram(sentence, idx)\n",
    "                if self.ngram_size == 1:\n",
    "                    entropy.append(np.log2(self.get_unigram_probability(ngram)))\n",
    "                else:\n",
    "                    entropy.append(np.log2(self.get_ngram_probability(ngram[0], ngram[1])))\n",
    "        \n",
    "        avg_entropy = -1 * (sum(entropy) / len(entropy))\n",
    "        \n",
    "        return pow(2.0, avg_entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "367.1742875616473"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example code to run a unigram model with add 0.001 smoothing. Tokens with a frequency count lower than 10\n",
    "# are replaced with the UNK string\n",
    "n = 1\n",
    "train_corpus = Corpus(train_path, 10, n, bos_eos=True, vocab=None)\n",
    "unigram_model = LM(n, lam=0.001)\n",
    "unigram_model.update_counts(train_corpus)\n",
    "\n",
    "# to ensure consistency, the test corpus is filtered using the vocabulary of the trained language model\n",
    "test_corpus = Corpus(test_path, None, n, bos_eos=True, vocab=unigram_model.vocab)\n",
    "unigram_model.perplexity(test_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting up LM\n",
      "Updating counts of LM\n",
      "LM update counts is DONE!\n",
      "Testing...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.4000165115330554"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example code to run a bigram model with add 0.001 smoothing. The same frequency threshold is applied.\n",
    "n = 2\n",
    "train_corpus = Corpus(train_path, 10, n, bos_eos=True, vocab=None)\n",
    "\n",
    "print(\"Starting up LM\")\n",
    "bigram_model = LM(n, lam=0.001)\n",
    "print(\"Updating counts of LM\")\n",
    "bigram_model.update_counts(train_corpus)\n",
    "print(\"LM update counts is DONE!\")\n",
    "\n",
    "# to ensure consistency, the test corpus is filtered using the vocabulary of the trained language model\n",
    "test_corpus = Corpus(test_path, None, n, bos_eos=True, vocab=bigram_model.vocab)\n",
    "print(\"Testing...\")\n",
    "bigram_model.perplexity(test_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes\n",
    "\n",
    "        KNESER NEY\n",
    "        PCONT CODE\n",
    "        \n",
    "        Number of different contexts the work appears in, aka, the number of bigrams it completes!\n",
    "        This way a frequent word that only appears in some specific contexts will have a low Pcont,\n",
    "        and thus not mess up estimates even if they're really frequent!\n",
    "        \n",
    "        wi : target word\n",
    "        \n",
    "        \n",
    "        nominator:    number of different string types preceding the final word, how many words appear \n",
    "                      before our word in a bigram?\n",
    "        denominator:  number of different possible n-gram types (or, simply put, the length of the \n",
    "                        n-gram table, all bigram types that appear mopre than once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
